{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Hyperparameter Optimization Project\n",
    "\n",
    "## Fall 2021 - Team 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you try to optimize hyperparameters ``C`` and ``solver`` of the classifier by using optuna.\n",
    "When you introduce optuna naively, you define an ``objective`` function\n",
    "such that it takes ``trial`` and calls ``suggest_*`` methods of ``trial`` to sample the hyperparameters:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_features=10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "C = 0.01\n",
    "clf = LogisticRegression(C=C)\n",
    "clf.fit(X_train, y_train)\n",
    "val_accuracy = clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space_C = [1e-7, 10.0]\n",
    "search_space_solver = [\"lbfgs\", \"saga\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Users's code\n",
    "\n",
    "def objective( C, solver):\n",
    "    X, y = make_classification(n_features=10)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "#     C = trial.suggest_loguniform(\"C\", 1e-7, 10.0)\n",
    "#     solver = trial.suggest_categorical(\"solver\", (\"lbfgs\", \"saga\"))\n",
    "\n",
    "    clf = LogisticRegression(C=C, solver=solver)\n",
    "    clf.fit(X_train, y_train)\n",
    "    val_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "    return val_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within API\n",
    "\n",
    "def create_study(C, solver): # these two are search space \n",
    "    # The function needs to be more generalizbale, later change required\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    trial = study.ask()\n",
    "    C = trial.suggest_loguniform(\"C\", C[0], C[1])\n",
    "    solver = trial.suggest_categorical(\"solver\", set(solver))\n",
    "    return C, solver, study,trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 16:37:12,624]\u001b[0m A new study created in memory with name: no-name-f0a08b32-a5ca-4266-9c3d-df809ac98590\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# needs Generalize \n",
    "C, solver, study, trial = create_study([1e-7, 10.0],[\"lbfgs\", \"saga\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<optuna.study.study.Study at 0x7f88f38d9af0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saga'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010015099407389486"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User's code\n",
    "performance = objective(C,solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<optuna.trial._trial.Trial at 0x7f88f084e790>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within API\n",
    "def update_study(performance, study,trial,search_space_C,search_space_solver ):\n",
    "    trial = study.ask()\n",
    "    C = trial.suggest_loguniform(\"C\", search_space_C[0], search_space_C[1])\n",
    "    solver = trial.suggest_categorical(\"solver\", set(search_space_solver))\n",
    "    study.tell(trial, performance)\n",
    "    return C, solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User's code\n",
    "new_C, new_solver = update_study(performance, study,trial,search_space_C,search_space_solver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2369180687810585"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saga'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance = objective(new_C, new_solver)\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. explanation docu (Jiayang)\n",
    "2. Expand to XGBoost (add input of XBoost algo and hyper params), generalize search_space (dict) (Jiayang, Deepak, Ash, Yuahan)\n",
    "3. wrapping up for simple use, reduce amount of data user need to input (Ashwat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interface is not flexible enough.\n",
    "For example, if ``objective`` requires additional arguments other than ``trial``,\n",
    "you need to define a class as in\n",
    "`How to define objective functions that have own arguments? <../../faq.html#how-to-define-objective-functions-that-have-own-arguments>`_.\n",
    "The ask-and-tell interface provides a more flexible syntax to optimize hyperparameters.\n",
    "The following example is equivalent to the previous code block.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "n_trials = 10\n",
    "for _ in range(n_trials):\n",
    "    trial = study.ask()  # `trial` is a `Trial` and not a `FrozenTrial`.\n",
    "\n",
    "    C = trial.suggest_loguniform(\"C\", 1e-7, 10.0)\n",
    "    solver = trial.suggest_categorical(\"solver\", (\"lbfgs\", \"saga\"))\n",
    "\n",
    "    clf = LogisticRegression(C=C, solver=solver)\n",
    "    clf.fit(X_train, y_train)\n",
    "    val_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "    study.tell(trial, val_accuracy)  # tell the pair of trial and objective value\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.suggest_loguniform(\"C\", 1e-7, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference is to use two methods: :func:`optuna.study.Study.ask`\n",
    "and :func:`optuna.study.Study.tell`.\n",
    ":func:`optuna.study.Study.ask` creates a trial that can sample hyperparameters, and\n",
    ":func:`optuna.study.Study.tell` finishes the trial by passing ``trial`` and an objective value.\n",
    "You can apply Optuna's hyperparameter optimization to your original code\n",
    "without an ``objective`` function.\n",
    "\n",
    "If you want to make your optimization faster with a pruner, you need to explicitly pass the state of trial\n",
    "to the argument of :func:`optuna.study.Study.tell` method as follows:\n",
    "\n",
    ".. code-block:: python\n",
    "\n",
    "   import numpy as np\n",
    "   from sklearn.datasets import load_iris\n",
    "   from sklearn.linear_model import SGDClassifier\n",
    "   from sklearn.model_selection import train_test_split\n",
    "\n",
    "   import optuna\n",
    "\n",
    "\n",
    "   X, y = load_iris(return_X_y=True)\n",
    "   X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "   classes = np.unique(y)\n",
    "   n_train_iter = 100\n",
    "\n",
    "   # define study with hyperband pruner.\n",
    "   study = optuna.create_study(\n",
    "       direction=\"maximize\",\n",
    "       pruner=optuna.pruners.HyperbandPruner(\n",
    "           min_resource=1, max_resource=n_train_iter, reduction_factor=3\n",
    "       ),\n",
    "   )\n",
    "\n",
    "   for _ in range(20):\n",
    "       trial = study.ask()\n",
    "\n",
    "       alpha = trial.suggest_uniform(\"alpha\", 0.0, 1.0)\n",
    "\n",
    "       clf = SGDClassifier(alpha=alpha)\n",
    "       pruned_trial = False\n",
    "\n",
    "       for step in range(n_train_iter):\n",
    "           clf.partial_fit(X_train, y_train, classes=classes)\n",
    "\n",
    "           intermediate_value = clf.score(X_valid, y_valid)\n",
    "           trial.report(intermediate_value, step)\n",
    "\n",
    "           if trial.should_prune():\n",
    "               pruned_trial = True\n",
    "               break\n",
    "\n",
    "       if pruned_trial:\n",
    "           study.tell(trial, state=optuna.trial.TrialState.PRUNED)  # tell the pruned state\n",
    "       else:\n",
    "           score = clf.score(X_valid, y_valid)\n",
    "           study.tell(trial, score)  # tell objective value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>:func:`optuna.study.Study.tell` method can take a trial number rather than the trial object.\n",
    "    ``study.tell(trial.number, y)`` is equivalent to ``study.tell(trial, y)``.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define-and-Run\n",
    "The ask-and-tell interface supports both `define-by-run` and `define-and-run` APIs.\n",
    "This section shows the example of the `define-and-run` API\n",
    "in addition to the define-by-run example above.\n",
    "\n",
    "Define distributions for the hyperparameters before calling the\n",
    ":func:`optuna.study.Study.ask` method for define-and-run API.\n",
    "For example,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = {\n",
    "    \"C\": optuna.distributions.LogUniformDistribution(1e-7, 10.0),\n",
    "    \"solver\": optuna.distributions.CategoricalDistribution((\"lbfgs\", \"saga\")),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass ``distributions`` to :func:`optuna.study.Study.ask` method at each call.\n",
    "The retuned ``trial`` contains the suggested hyperparameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 16:40:26,278]\u001b[0m A new study created in memory with name: no-name-f4657b05-ac84-4cd1-8494-0f979423be29\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "n_trials = 10\n",
    "for _ in range(n_trials):\n",
    "    trial = study.ask(distributions)  # pass the pre-defined distributions.\n",
    "\n",
    "    # two hyperparameters are already sampled from the pre-defined distributions\n",
    "    C = trial.params[\"C\"]\n",
    "    solver = trial.params[\"solver\"]\n",
    "\n",
    "    clf = LogisticRegression(C=C, solver=solver)\n",
    "    clf.fit(X_train, y_train)\n",
    "    val_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "    study.tell(trial, val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Batch Optimization\n",
    "The ask-and-tell interface enables us to optimize a batched objective for faster optimization.\n",
    "For example, parallelizable evaluation, operation over vectors, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following objective takes batched hyperparameters ``xs`` and ``ys`` instead of a single\n",
    "pair of hyperparameters ``x`` and ``y`` and calculates the objective over the full vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_objective(xs: np.ndarray, ys: np.ndarray):\n",
    "    return xs ** 2 + ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, the number of pairs of hyperparameters in a batch is $10$,\n",
    "and ``batched_objective`` is evaluated three times.\n",
    "Thus, the number of trials is $30$.\n",
    "Note that you need to store either ``trial_ids`` or ``trial`` to call\n",
    ":func:`optuna.study.Study.tell` method after the batched evaluations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 16:40:27,416]\u001b[0m A new study created in memory with name: no-name-ff5cc61d-9c89-4368-9dc9-77449575bc44\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "study = optuna.create_study(sampler=optuna.samplers.CmaEsSampler())\n",
    "\n",
    "for _ in range(3):\n",
    "\n",
    "    # create batch\n",
    "    trial_ids = []\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for _ in range(batch_size):\n",
    "        trial = study.ask()\n",
    "        trial_ids.append(trial.number)\n",
    "        x_batch.append(trial.suggest_float(\"x\", -10, 10))\n",
    "        y_batch.append(trial.suggest_float(\"y\", -10, 10))\n",
    "\n",
    "    # evaluate batched objective\n",
    "    x_batch = np.array(x_batch)\n",
    "    y_batch = np.array(y_batch)\n",
    "    objectives = batched_objective(x_batch, y_batch)\n",
    "\n",
    "    # finish all trials in the batch\n",
    "    for trial_id, objective in zip(trial_ids, objectives):\n",
    "        study.tell(trial_id, objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
