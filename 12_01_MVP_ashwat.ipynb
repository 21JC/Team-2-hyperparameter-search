{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Hyperparameter Optimization Project\n",
    "\n",
    "## Fall 2021 - Team 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you try to optimize hyperparameters ``C`` and ``solver`` of the classifier by using optuna.\n",
    "When you introduce optuna naively, you define an ``objective`` function\n",
    "such that it takes ``trial`` and calls ``suggest_*`` methods of ``trial`` to sample the hyperparameters:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# building a Python class for project\n",
    "\n",
    "class Tune:\n",
    "    study_name = None\n",
    "    search_space_C = None\n",
    "    search_space_solver = None\n",
    "    study = None\n",
    "    trial = None\n",
    "    objective = None\n",
    "    performance = None\n",
    "    stopping_criteria = None\n",
    "    C = None\n",
    "    solver = None\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.study_name = name\n",
    "    \n",
    "    def create_study(self, C, solver):\n",
    "        self.search_space_C = C\n",
    "        self.search_space_solver = solver\n",
    "        \n",
    "        # The function needs to be more generalizbale, later change required\n",
    "        self.study = optuna.create_study(direction=\"maximize\", study_name=self.study_name)\n",
    "        self.trial = self.study.ask()\n",
    "        self.C = self.trial.suggest_loguniform(\"C\", self.search_space_C[0], self.search_space_C[1])\n",
    "        self.solver = self.trial.suggest_categorical(\"solver\", set(self.search_space_solver))\n",
    "#         print(\"Hyperparameter:\", self.C)\n",
    "#         print(\"Solver:\", self.solver)\n",
    "    \n",
    "    # get single update from study\n",
    "    def update_study(self, n=1, output=True):\n",
    "        self.get_performance()\n",
    "        self.trial = self.study.ask()\n",
    "        self.C = self.trial.suggest_loguniform(\"C\", self.search_space_C[0], self.search_space_C[1])\n",
    "        self.solver = self.trial.suggest_categorical(\"solver\", set(self.search_space_solver))\n",
    "        self.study.tell(self.trial, self.performance)\n",
    "    \n",
    "        # push data using Flask for updating SQL table\n",
    "        study = {\"study-name\" : self.study_name,\n",
    "         \"search-space-hyperparameter\" : str(self.search_space_C),\n",
    "         \"search-space-solver\" : str(self.search_space_solver),\n",
    "         \"hyperparameter\" : self.C,\n",
    "         \"solver\" : self.solver,\n",
    "         \"stopping-criteria\" : self.stopping_criteria,\n",
    "         \"num-updates\" : n,\n",
    "         \"performance\" : self.performance}\n",
    "        res = requests.post('http://localhost:5000///updateDB', json=study)\n",
    "        \n",
    "        if output:\n",
    "            print(\"Hyperparameter:\", self.C)\n",
    "            print(\"Solver:\", self.solver)\n",
    "            print(\"Performance:\", self.performance)\n",
    "    \n",
    "    def update_study_repeat(self, n):\n",
    "        for _ in range(n):\n",
    "            self.update_study(n, False)\n",
    "            \n",
    "        # retrieve best study data from SQL table using Flask\n",
    "        study = {\"study-name\" : self.study_name}\n",
    "        res = requests.post('http://localhost:5000///getBestStudy', json=study).json()\n",
    "        \n",
    "        print(\"Best Results Across Study - \", self.study_name)\n",
    "        print(\"Best Hyperparameters:\", res['hyperparameter'])\n",
    "        print(\"Best Solver:\", res[\"solver\"])\n",
    "        print(\"Best Model Performance:\", res['performance'])\n",
    "    \n",
    "    # set objective function for study\n",
    "    def set_objective(self, objective):\n",
    "        self.objective = objective\n",
    "        print(\"Objective has been set.\")\n",
    "    \n",
    "    # retrieve current performance data using current recommended hyperparameter and solver\n",
    "    def get_performance(self):\n",
    "        self.performance = self.objective(self.C, self.solver)\n",
    "        return self.performance\n",
    "#         print(\"Current Model Performance:\", self.performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defines data and split\n",
    "\n",
    "X, y = make_classification(n_features=10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defines objective function\n",
    "\n",
    "def objective(C, solver):\n",
    "    clf = LogisticRegression(C=C, solver=solver)\n",
    "    clf.fit(X_train, y_train)\n",
    "    val_accuracy = clf.score(X_test, y_test)\n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune = Tune(\"winston_test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 19:38:48,906]\u001b[0m A new study created in memory with name: winston_test2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tune.create_study([1e-7, 10.0], [\"lbfgs\", \"saga\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective has been set.\n"
     ]
    }
   ],
   "source": [
    "tune.set_objective(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter: 1.027081406050604e-05\n",
      "Solver: saga\n",
      "Performance: 0.44\n"
     ]
    }
   ],
   "source": [
    "tune.update_study()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Results Across Study -  winston_test2\n",
      "Best Hyperparameters: 0.000004482307998114341\n",
      "Best Solver: lbfgs\n",
      "Best Model Performance: 0.920000000\n"
     ]
    }
   ],
   "source": [
    "tune.update_study_repeat(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
